"example": 1,
"id": "GHSA-8fxw-76px-3rxv",
"title": "Memory leak in Tensorflow",
"advisory_link": "https://github.com/advisories/GHSA-8fxw-76px-3rxv",    
"vulnerable_code_highlighted":"
  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);
  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);
",
"vulnerable_code_inside_full_function":"
void* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {
  const Tensor* tensor = GetTensorFromHandle(h, status);
  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());
  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()

  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);
  tf_dlm_tensor_ctx->reference = tensor_ref;

  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;
  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;
  dlm_tensor->deleter = &DLManagedTensorDeleter;
  dlm_tensor->dl_tensor.ctx = GetDlContext(h, status);
  int ndim = tensor->dims();
  dlm_tensor->dl_tensor.ndim = ndim;
  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);
  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);

  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;
  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;
  shape_arr->resize(ndim);
  stride_arr->resize(ndim, 1);
  for (int i = 0; i < ndim; i++) {
    (*shape_arr)[i] = tensor->dim_size(i);
  }
  for (int i = ndim - 2; i >= 0; --i) {
    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];
  }

  dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];
  // There are two ways to represent compact row-major data
  // 1) nullptr indicates tensor is compact and row-majored.
  // 2) fill in the strides array as the real case for compact row-major data.
  // Here we choose option 2, since some frameworks didn't handle the strides
  // argument properly.
  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];
  dlm_tensor->dl_tensor.byte_offset =
      0;  // TF doesn't handle the strides and byte_offsets here
  return static_cast<void*>(dlm_tensor);
}
",
"revised_patch_highlighted":
" auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);
  if (!status->status.ok()) {
    return nullptr;
  }
  auto tf_dlm_type = GetDlDataType(data_type, status);
  if (!status->status.ok()) {
    return nullptr;
  }

  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()
  dlm_tensor->dl_tensor.data = tf_dlm_data;
  dlm_tensor->dl_tensor.dtype = tf_dlm_type;
  "
------------------------------
"example": 2,
"id": "GHSA-9mqp-7v2h-2382",
"title": "Denial of Service in Tensorflow",
"advisory_link": "https://github.com/advisories/GHSA-9mqp-7v2h-2382",    
"vulnerable_code_highlighted":"
OP_REQUIRES(
    context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),
    errors::InvalidArgument("reverse_index_map must be a vector, saw: ",
                            reverse_index_map_t->shape().DebugString()));

const auto reverse_index_map = reverse_index_map_t->vec<int64>();
const auto grad_values = grad_values_t->vec<T>();

d_values(i) = grad_values(reverse_index_map(i));
visited(reverse_index_map(i)) = true;
",
"vulnerable_code_inside_full_function":"
  void Compute(OpKernelContext* context) override {
const Tensor* reverse_index_map_t;
const Tensor* grad_values_t;
OP_REQUIRES_OK(context,
               context->input("reverse_index_map", &reverse_index_map_t));
OP_REQUIRES_OK(context, context->input("grad_values", &grad_values_t));

const CPUDevice& d = context->eigen_device<CPUDevice>();

OP_REQUIRES(
    context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),
    errors::InvalidArgument("reverse_index_map must be a vector, saw: ",
                            reverse_index_map_t->shape().DebugString()));

const auto reverse_index_map = reverse_index_map_t->vec<int64>();
const auto grad_values = grad_values_t->vec<T>();

const int64 N = reverse_index_map_t->shape().dim_size(0);
const int64 N_full = grad_values_t->shape().dim_size(0);

Tensor* d_values_t;
OP_REQUIRES_OK(context, context->allocate_output(
                            "d_values", TensorShape({N}), &d_values_t));
auto d_values = d_values_t->vec<T>();
Tensor* d_default_value_t;
OP_REQUIRES_OK(context,
               context->allocate_output("d_default_value", TensorShape({}),
                                        &d_default_value_t));
T& d_default_value = d_default_value_t->scalar<T>()();
d_default_value = T();

Tensor visited_t;
OP_REQUIRES_OK(context, context->allocate_temp(
                            DT_BOOL, TensorShape({N_full}), &visited_t));
auto visited = visited_t.vec<bool>();
visited.device(d) = visited.constant(false);

for (int i = 0; i < N; ++i) {
  // Locate the index of the output of the forward prop associated
  // with this location in the input of the forward prop.  Copy
  // the gradient into it.  Mark it as visited.
  d_values(i) = grad_values(reverse_index_map(i));
  visited(reverse_index_map(i)) = true;
}
for (int j = 0; j < N_full; ++j) {
  // The default value gradient gets the accumulated remainder of
  // the backprop values (since the default value was used to fill
  // in these slots in the forward calculation).
  if (!visited(j)) {
    d_default_value += grad_values(j);
  }
}",
"revised_patch_highlighted":"
OP_REQUIRES(context, TensorShapeUtils::IsVector(grad_values_t->shape()),
            errors::InvalidArgument("grad_values must be a vector, saw: ",
                                    grad_values_t->shape().DebugString()));

int64 reverse_index = reverse_index_map(i);
OP_REQUIRES(
  context, 0 <= reverse_index && reverse_index < N_full,
  errors::InvalidArgument("Elements in reverse index must be in [0, ",
                          N_full, ") but got ", reverse_index));
d_values(i) = grad_values(reverse_index);
visited(reverse_index) = true;

"
------------------------------
"example": 3,
"id": "GHSA-4g9f-63rx-5cw4",
"title": "Segfault in Tensorflow",
"advisory_link": "https://github.com/advisories/GHSA-4g9f-63rx-5cw4",    
"vulnerable_code_highlighted":"
  if (outputs != nullptr) {
    outputs->clear();
    for (int i = 0; i < context.num_outputs(); ++i) {
      outputs->push_back(Tensor(*context.mutable_output(i)));
    }
  }
",
"vulnerable_code_inside_full_function":"
    Status KernelAndDeviceOp::Run(
    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,
    std::vector<EagerKernelRet>* outputs,
    CancellationManager* cancellation_manager,
    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {
	  OpKernelContext::Params params;
	  params.device = device_;
	  params.frame_iter = FrameAndIter(0, 0);
	  params.inputs = inputs.GetTensorValues();
	  params.op_kernel = kernel_.get();
	  params.resource_manager = device_->resource_manager();
	  params.input_alloc_attrs = &input_alloc_attrs_;
	  params.output_attr_array = output_alloc_attrs_.data();
	  params.function_library = flr_;
	  params.slice_reader_cache = &slice_reader_cache_;
	  params.rendezvous = rendezvous_;
	  OpExecutionState* op_execution_state = nullptr;

	  CancellationManager default_cancellation_manager;
	  if (cancellation_manager) {
	    params.cancellation_manager = cancellation_manager;
	  } else if (kernel_->is_deferred()) {
	    op_execution_state = new OpExecutionState;
	    params.cancellation_manager = &op_execution_state->cancellation_manager;
	    params.inc_num_deferred_ops_function = [op_execution_state]() {
	      op_execution_state->Ref();
	    };
	    params.dec_num_deferred_ops_function = [op_execution_state]() {
	      op_execution_state->Unref();
	    };
	  } else {
	    params.cancellation_manager = &default_cancellation_manager;
	  }

	  params.log_memory = log_memory_;

	  params.runner = get_runner();

	  params.step_container =
	      step_container == nullptr ? &step_container_ : step_container;
	  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {
	    if (step_container == nullptr) {
	      this->step_container_.CleanUp();
	    }
	  });

	  params.collective_executor =
	      collective_executor_ ? collective_executor_->get() : nullptr;

	  OpKernelContext context(&params);

	  {
	    port::ScopedFlushDenormal flush;
	    port::ScopedSetRound round(FE_TONEAREST);
	    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution
	    // time on device of the OpKernel.
	    profiler::AnnotatedTraceMe activity(
	        [&] { return kernel_->TraceString(context, /*verbose=*/false); },
	        profiler::TraceMeLevel::kInfo);
	    device_->Compute(kernel_.get(), &context);
	  }

	  // Clean up execution op_execution_state if deferred ops aren't running.
	  if (op_execution_state != nullptr) {
	    op_execution_state->Unref();
	  }

	  if (!context.status().ok()) return context.status();

	  if (outputs != nullptr) {
	    outputs->clear();
	    for (int i = 0; i < context.num_outputs(); ++i) {
	      outputs->push_back(Tensor(*context.mutable_output(i)));
	    }
	  }
	  return Status::OK();
	}
",
"revised_patch_highlighted":"
	const auto* output_tensor = context.mutable_output(i);
      if (output_tensor != nullptr) {
        outputs->push_back(Tensor(*output_tensor));
      } else {
        outputs->push_back(Tensor());
      }"
------------------------------
"example": 4,
"id": "GHSA-jc87-6vpp-7ff3",
"title": "Heap buffer overflow in Tensorflow",
"advisory_link": "https://github.com/advisories/GHSA-jc87-6vpp-7ff3",    
"vulnerable_code_highlighted":"
for (int idx = 0; idx < num_values; ++idx) {
  int batch = is_1d ? 0 : indices_values(idx, 0);
  const auto& value = values_values(idx);
",
"vulnerable_code_inside_full_function":"
void Compute(OpKernelContext* context) override {
	const Tensor& indices = context->input(0);
	const Tensor& values = context->input(1);
	const Tensor& shape = context->input(2);
	const Tensor& weights = context->input(3);
	bool use_weights = weights.NumElements() > 0;

	bool is_1d = shape.NumElements() == 1;
	int num_batches = is_1d ? 1 : shape.flat<int64>()(0);
	int num_values = values.NumElements();

	const auto indices_values = indices.matrix<int64>();
	const auto values_values = values.flat<T>();
	const auto weight_values = weights.flat<W>();

	auto per_batch_counts = BatchedMap<W>(num_batches);

	T max_value = 0;

	for (int idx = 0; idx < num_values; ++idx) {
	  int batch = is_1d ? 0 : indices_values(idx, 0);
	  const auto& value = values_values(idx);
	  if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
	    if (binary_output_) {
	      per_batch_counts[batch][value] = 1;
	    } else if (use_weights) {
	      per_batch_counts[batch][value] += weight_values(idx);
	    } else {
	      per_batch_counts[batch][value]++;
	    }
	    if (value > max_value) {
	      max_value = value;
	    }
	  }
	}

	int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);
	OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,
                                        is_1d, context));
}
",
"revised_patch_highlighted":"
    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),
                errors::InvalidArgument(
                    "Input indices must be a 2-dimensional tensor. Got: ",
                    indices.shape().DebugString()));

    if (use_weights) {
      OP_REQUIRES(
          context, weights.shape() == values.shape(),
          errors::InvalidArgument(
              "Weights and values must have the same shape. Weight shape: ",
              weights.shape().DebugString(),
              "; values shape: ", values.shape().DebugString()));
    }

    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),
            errors::InvalidArgument(
                "Number of values must match first dimension of indices.",
                "Got ", num_values,
                " values, indices shape: ", indices.shape().DebugString()));

    "
------------------------------
"example": 5,
"id": "GHSA-q8qj-fc9q-cphr",
"title": "Undefined behavior in Tensorflow",
"advisory_link": "https://github.com/advisories/GHSA-q8qj-fc9q-cphr",    
"vulnerable_code_highlighted":"
dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);
	dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);
",
"vulnerable_code_inside_full_function":"
void* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {
	  const Tensor* tensor = GetTensorFromHandle(h, status);
	  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());
	  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()

	  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);
	  tf_dlm_tensor_ctx->reference = tensor_ref;

	  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;
	  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;
	  dlm_tensor->deleter = &DLManagedTensorDeleter;
	  dlm_tensor->dl_tensor.ctx = GetDlContext(h, status);
	  int ndim = tensor->dims();
	  dlm_tensor->dl_tensor.ndim = ndim;
	  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);
	  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);

	  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;
	  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;
	  shape_arr->resize(ndim);
	  stride_arr->resize(ndim, 1);
	  for (int i = 0; i < ndim; i++) {
	    (*shape_arr)[i] = tensor->dim_size(i);
	  }
	  for (int i = ndim - 2; i >= 0; --i) {
	    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];
	  }

	  dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];
	  // There are two ways to represent compact row-major data
	  // 1) nullptr indicates tensor is compact and row-majored.
	  // 2) fill in the strides array as the real case for compact row-major data.
	  // Here we choose option 2, since some frameworks didn't handle the strides
	  // argument properly.
	  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];
	  dlm_tensor->dl_tensor.byte_offset =
	      0;  // TF doesn't handle the strides and byte_offsets here
	  return static_cast<void*>(dlm_tensor);
}
",
"revised_patch_highlighted":"
  auto tf_dlm_context = GetDlContext(h, status);
  if (!status->status.ok()) {
    return nullptr;
  }

  auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);
  if (!status->status.ok()) {
    return nullptr;
  }

  auto tf_dlm_type = GetDlDataType(data_type, status);
  if (!status->status.ok()) {
    return nullptr;
  }

  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()

  dlm_tensor->dl_tensor.ctx = tf_dlm_context;
  dlm_tensor->dl_tensor.data = tf_dlm_data;
  dlm_tensor->dl_tensor.dtype = tf_dlm_type;
"
------------------------------
	"example": 6,
    "id": "GHSA-qh32-6jjc-qprm",
    "title": "Null pointer dereference in tensorflow-lite",
    "advisory_link": "https://github.com/advisories/GHSA-qh32-6jjc-qprm",    
    "vulnerable_code_highlighted":"
      TfLiteTensorReset(type, name, ConvertArrayToTfLiteIntArray(rank, dims),
                    GetLegacyQuantization(quantization),
                    /*buffer=*/nullptr, required_bytes, allocation_type,
                    nullptr, is_variable, &tensor);
    ",
    "vulnerable_code_inside_full_function":"
    TfLiteStatus Subgraph::SetTensorParametersReadWrite(
    int tensor_index, TfLiteType type, const char* name, const size_t rank,
    const int* dims, TfLiteQuantization quantization, bool is_variable,
    const size_t rank_dims_signature, const int* dims_signature) {
	  // Ensure quantization cleanup on failure.
	  ScopedTfLiteQuantization scoped_quantization(&quantization);
	  if (state_ == kStateInvokableAndImmutable) {
	    ReportError(
	        "SetTensorParametersReadWrite is disallowed when graph is immutable.");
	    return kTfLiteError;
	  }
	  TF_LITE_ENSURE(&context_,
	                 tensor_index < context_.tensors_size && tensor_index >= 0);
	  size_t required_bytes = 0;
	  if (type != kTfLiteString) {
	    // These types will be allocated in our arena so we need to record how
	    // many bytes we will need based on the dimensions. String tensors are
	    // allocated dynamically and we can't know ahead of time how much space
	    // they will require.
	    TF_LITE_ENSURE_OK(&context_,
	                      BytesRequired(type, dims, rank, &required_bytes));
	  }

	  TfLiteAllocationType allocation_type = kTfLiteArenaRw;
	  if (type == kTfLiteString) {
	    if (is_variable) {
	      // We don't have a real use case for string variable tensor.
	      ReportError("String variable tensor isn't supported.");
	      return kTfLiteError;
	    }
	    allocation_type = kTfLiteDynamic;
	  } else if (is_variable) {
	    allocation_type = kTfLiteArenaRwPersistent;
	  }

	  TfLiteTensor& tensor = context_.tensors[tensor_index];
	  TfLiteTensorReset(type, name, ConvertArrayToTfLiteIntArray(rank, dims),
	                    GetLegacyQuantization(quantization),
	                    /*buffer=*/nullptr, required_bytes, allocation_type,
	                    nullptr, is_variable, &tensor);
	  // TODO(suharshs): Update TfLiteTensorReset to include the new quantization
	  // if there are other required callers.
	  tensor.quantization = *scoped_quantization.release();
	  tensor.dims_signature =
	      ConvertArrayToTfLiteIntArray(rank_dims_signature, dims_signature);
	  return kTfLiteOk;
	}
	",
	"revised_patch_highlighted":""
------------------------------
	"example": 7,
    "id": "GHSA-q8gv-q7wr-9jf8",
    "title": "Segfault in Tensorflow",
    "advisory_link": "https://github.com/advisories/GHSA-q8gv-q7wr-9jf8",    
    "vulnerable_code_highlighted":"
    int64 id = ctx->session_state()->GetNewId();
    ",
    "vulnerable_code_inside_full_function":"
	    void Compute(OpKernelContext* ctx) override {
	    const Tensor& val = ctx->input(0);
	    int64 id = ctx->session_state()->GetNewId();
	    TensorStore::TensorAndKey tk{val, id, requested_device()};
	    OP_REQUIRES_OK(ctx, ctx->tensor_store()->AddTensor(name(), tk));

	    Tensor* handle = nullptr;
	    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));
	    if (ctx->expected_output_dtype(0) == DT_RESOURCE) {
	      ResourceHandle resource_handle = MakeResourceHandle<Tensor>(
	          ctx, SessionState::kTensorHandleResourceTypeName,
	          tk.GetHandle(name()));
	      resource_handle.set_maybe_type_name(
	          SessionState::kTensorHandleResourceTypeName);
	      handle->scalar<ResourceHandle>()() = resource_handle;
	    } else {
	      // Legacy behavior in V1.
	      handle->flat<tstring>().setConstant(tk.GetHandle(name()));
	    }
	  }
	",
	"revised_patch_highlighted":""
------------------------------
	"example": 8,
    "id": "GHSA-hjmq-236j-8m87",
    "title": "Denial of service in tensorflow-lite",
    "advisory_link": "https://github.com/advisories/GHSA-hjmq-236j-8m87",    
    "vulnerable_code_highlighted":"
	  if (segment_id_size > 0) {
	    max_index = segment_ids->data.i32[segment_id_size - 1];
	  }
	  const int data_rank = NumDimensions(data);
	  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));
	  output_shape->data[0] = max_index + 1;
    ",
    "vulnerable_code_inside_full_function":"
    TfLiteStatus ResizeOutputTensor(TfLiteContext* context,
                                const TfLiteTensor* data,
                                const TfLiteTensor* segment_ids,
                                TfLiteTensor* output) {
	  int max_index = -1;
	  const int segment_id_size = segment_ids->dims->data[0];
	  if (segment_id_size > 0) {
	    max_index = segment_ids->data.i32[segment_id_size - 1];
	  }
	  const int data_rank = NumDimensions(data);
	  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));
	  output_shape->data[0] = max_index + 1;
	  for (int i = 1; i < data_rank; ++i) {
	    output_shape->data[i] = data->dims->data[i];
	  }
	  return context->ResizeTensor(context, output, output_shape);
	}
	",
	"revised_patch_highlighted":""
------------------------------
    "example": 9,
    "id": "GHSA-x7rp-74x2-mjf3",
    "title": "Segfault in Tensorflow",
    "advisory_link": "https://github.com/advisories/GHSA-x7rp-74x2-mjf3",    
    "vulnerable_code_highlighted":"
    for (int idx = 0; idx < num_values; ++idx) {
      while (idx >= splits_values(batch_idx)) {
        batch_idx++;
      }
      const auto& value = values_values(idx);
      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
        if (binary_output_) {
          per_batch_counts[batch_idx - 1][value] = 1;
        } else if (use_weights) {
          per_batch_counts[batch_idx - 1][value] += weight_values(idx);
        } else {
          per_batch_counts[batch_idx - 1][value]++;
        }
        if (value > max_value) {
          max_value = value;
        }
      }
    }
    ",
    "vulnerable_code_inside_full_function":"

     void Compute(OpKernelContext* context) override {
	    const Tensor& splits = context->input(0);
	    const Tensor& values = context->input(1);
	    const Tensor& weights = context->input(2);
	    bool use_weights = weights.NumElements() > 0;
	    bool is_1d = false;

	    const auto splits_values = splits.flat<int64>();
	    const auto values_values = values.flat<T>();
	    const auto weight_values = weights.flat<W>();
	    int num_batches = splits.NumElements() - 1;
	    int num_values = values.NumElements();

	    auto per_batch_counts = BatchedMap<W>(num_batches);
	    T max_value = 0;
	    int batch_idx = 0;

	    for (int idx = 0; idx < num_values; ++idx) {
	      while (idx >= splits_values(batch_idx)) {
	        batch_idx++;
	      }
	      const auto& value = values_values(idx);
	      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
	        if (binary_output_) {
	          per_batch_counts[batch_idx - 1][value] = 1;
	        } else if (use_weights) {
	          per_batch_counts[batch_idx - 1][value] += weight_values(idx);
	        } else {
	          per_batch_counts[batch_idx - 1][value]++;
	        }
	        if (value > max_value) {
	          max_value = value;
	        }
	      }
	    }

	    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);
	    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,
	                                            is_1d, context));
	  }
	",
	"revised_patch_highlighted":""
------------------------------
    "example": 10,
    "id": "GHSA-x5cp-9pcf-pp3h",
    "title": "Denial of Service in Tensorflow",
    "advisory_link": "https://github.com/advisories/GHSA-x5cp-9pcf-pp3h",    
    "vulnerable_code_highlighted":"
    int num_batches = splits.NumElements() - 1;
    int num_values = values.NumElements();

    auto per_batch_counts = BatchedMap<W>(num_batches);
    ",
    "vulnerable_code_inside_full_function":"
      void Compute(OpKernelContext* context) override {
	    const Tensor& splits = context->input(0);
	    const Tensor& values = context->input(1);
	    const Tensor& weights = context->input(2);
	    bool use_weights = weights.NumElements() > 0;
	    bool is_1d = false;

	    const auto splits_values = splits.flat<int64>();
	    const auto values_values = values.flat<T>();
	    const auto weight_values = weights.flat<W>();
	    int num_batches = splits.NumElements() - 1;
	    int num_values = values.NumElements();

	    auto per_batch_counts = BatchedMap<W>(num_batches);
	    T max_value = 0;
	    int batch_idx = 0;

	    for (int idx = 0; idx < num_values; ++idx) {
	      while (idx >= splits_values(batch_idx)) {
	        batch_idx++;
	      }
	      const auto& value = values_values(idx);
	      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
	        if (binary_output_) {
	          per_batch_counts[batch_idx - 1][value] = 1;
	        } else if (use_weights) {
	          per_batch_counts[batch_idx - 1][value] += weight_values(idx);
	        } else {
	          per_batch_counts[batch_idx - 1][value]++;
	        }
	        if (value > max_value) {
	          max_value = value;
	        }
	      }
	    }

	    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);
	    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,
	                                            is_1d, context));
	  }
	",
	"revised_patch_highlighted":""
------------------------------
	"example": 11,
    "id": "GHSA-p2cq-cprg-frvm",
    "title": "Out of bounds write in TFLite implementation of segment sum",
    "advisory_link": "https://github.com/advisories/GHSA-p2cq-cprg-frvm",    
    "vulnerable_code_highlighted":"
	int max_index = -1;
	const int segment_id_size = segment_ids->dims->data[0];
	if (segment_id_size > 0) {
	  max_index = segment_ids->data.i32[segment_id_size - 1];
	}
	const int data_rank = NumDimensions(data);
	TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));
	output_shape->data[0] = max_index + 1;
    ",
    "vulnerable_code_inside_full_function":"
		TfLiteStatus ResizeOutputTensor(TfLiteContext* context,
										const TfLiteTensor* data,
										const TfLiteTensor* segment_ids,
										TfLiteTensor* output) {
		int max_index = -1;
		const int segment_id_size = segment_ids->dims->data[0];
		if (segment_id_size > 0) {
			max_index = segment_ids->data.i32[segment_id_size - 1];
		}
		const int data_rank = NumDimensions(data);
		TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));
		output_shape->data[0] = max_index + 1;
		for (int i = 1; i < data_rank; ++i) {
			output_shape->data[i] = data->dims->data[i];
		}
		return context->ResizeTensor(context, output, output_shape);
		}
	",
	"revised_patch_highlighted":"
		const int segment_id_size = segment_ids->dims->data[0];
		TF_LITE_ENSURE_EQ(context, segment_id_size, data->dims->data[0]);
		int previous_segment_id = -1;
		for (int i = 0; i < segment_id_size; i++) {
		const int current_segment_id = GetTensorData<int32_t>(segment_ids)[i];
		if (i == 0) {
			TF_LITE_ENSURE_EQ(context, current_segment_id, 0);
		} else {
			int delta = current_segment_id - previous_segment_id;
			TF_LITE_ENSURE(context, delta == 0 || delta == 1);
		}
		previous_segment_id = current_segment_id;
		}
	
		const int max_index = previous_segment_id;
	
		const int data_rank = NumDimensions(data);
	
	"
------------------------------	
	"example": 12,
    "id": "GHSA-h6fg-mjxg-hqq4",
    "title": "Integer truncation in Shard API usage",
    "advisory_link": "https://github.com/advisories/GHSA-h6fg-mjxg-hqq4",    
    "vulnerable_code_highlighted":"
		auto DoWork = [samples_per_alpha, num_alphas, &rng, samples_flat, 
			alpha_flat](int start_output, int limit_output) {
    ",
    "vulnerable_code_inside_full_function":"

		auto DoWork = [samples_per_alpha, num_alphas, &rng, samples_flat,
						alpha_flat](int start_output, int limit_output) {
			using Eigen::numext::exp;
			using Eigen::numext::log;
			using Eigen::numext::log1p;
			using Eigen::numext::pow;

			// Capturing "rng" by-value would only make a copy for the _shared_
			// lambda.  Since we want to let each worker have its own copy, we pass
			// "rng" by reference and explicitly do a copy assignment.

			Normal normal;
			Uniform uniform;
			typename Normal::ResultType norm_result;
			typename Uniform::ResultType uniform_result;
			for (int64 output_idx = start_output; output_idx < limit_output;
			/* output_idx incremented within inner loop below */) {
			int64 alpha_idx = output_idx / samples_per_alpha;

			// Instead of +alpha_idx for each sample, we offset the pointer once.
			T* const samples_alpha_offset = samples_flat + alpha_idx;

			// Several calculations can be done on a per-alpha basis.
			const double alpha = static_cast<double>(alpha_flat[alpha_idx]);

			DISABLE_FLOAT_EQUALITY_WARNING
			if (alpha == static_cast<double>(1.0)) {
			ENABLE_FLOAT_EQUALITY_WARNING
			// Sample from an exponential distribution.
			for (int64 sample_idx = output_idx % samples_per_alpha;
				sample_idx < samples_per_alpha && output_idx < limit_output;
				sample_idx++, output_idx++) {
			// As we want data stable regardless of sharding
			// (including eventually on GPU), we skip on a per-sample basis.
			PhiloxRandom gen = rng;
			gen.Skip(kReservedSamplesPerOutput * output_idx);
			int16 uniform_remaining = 0;
			UNIFORM(u);
			const double res = -log1p(-u);
			samples_alpha_offset[sample_idx * num_alphas] = static_cast<T>(res);
			}       // for (sample_idx)
			} else {  // if alpha != 1.0
			// Transformation-rejection from pairs of uniform and normal random
			// variables. http://dl.acm.org/citation.cfm?id=358414
			//
			// The algorithm has an acceptance rate of ~95% for small alpha (~1),
			// and higher accept rates for higher alpha, so runtime is
			// O(NumAlphas * NumSamples * k) with k ~ 1 / 0.95.
			//
			// For alpha<1, we add one to d=alpha-1/3, and multiply the final
			// result by uniform()^(1/alpha)
			const bool alpha_less_than_one = alpha < 1;
			const double d = alpha + (alpha_less_than_one ? 2.0 / 3 : -1.0 / 3);
			const double c = 1.0 / 3 / sqrt(d);

			// Compute the rest of the samples for the current alpha value.
			for (int64 sample_idx = output_idx % samples_per_alpha;
				sample_idx < samples_per_alpha && output_idx < limit_output;
				sample_idx++, output_idx++) {
			// Since each sample may use a variable number of normal/uniform
			// samples, and we want data stable regardless of sharding
			// (including eventually on GPU), we skip on a per-sample basis.
			PhiloxRandom gen = rng;
			gen.Skip(kReservedSamplesPerOutput * output_idx);
			int16 norm_remaining = 0;
			int16 uniform_remaining = 0;

			// Keep trying until we don't reject a sample. In practice, we will
			// only reject ~5% at worst, for low alpha near 1.
			while (true) {
			if (norm_remaining == 0) {
				norm_remaining = Normal::kResultElementCount;
				norm_result = normal(&gen);
			}
			norm_remaining--;
			const double x = norm_result[norm_remaining];
			double v = 1 + c * x;
			if (v <= 0) {
				continue;
			}
			v = v * v * v;
			UNIFORM(u);
			// The first option in the if is a "squeeze" short-circuit to
			// dodge the two logs. Magic constant sourced from the paper
			// linked above. Upward of .91 of the area covered by the log
			// inequality is covered by the squeeze as well (larger coverage
			// for smaller values of alpha).
			if ((u < 1 - 0.0331 * (x * x) * (x * x)) ||
				(log(u) < 0.5 * x * x + d * (1 - v + log(v)))) {
				double res = d * v;
				if (alpha_less_than_one) {
				UNIFORM(b);
				res *= pow(b, 1 / alpha);
				}
				samples_alpha_offset[sample_idx * num_alphas] =
					static_cast<T>(res);
				break;
			}
			}  // while: true
			}    // for: sample_idx
			}      // if (alpha == 1.0)
			}        // for: output_idx
			};         // DoWork
			
			
		",
	"revised_patch_highlighted":"
		auto DoWork = [samples_per_alpha, num_alphas, &rng, samples_flat,
			alpha_flat](int64 start_output, int64 limit_output) {
	"

------------------------------

------------------------------
	"example": 14,
	"id": "GHSA-cvpc-8phh-8f45",
	"title": "Out of bounds access in TFLite operators",
	"advisory_link": "https://github.com/advisories/GHSA-cvpc-8phh-8f45",    
	"vulnerable_code_highlighted":"
		if (context->tensors != nullptr) {
			return &context->tensors[node->outputs->data[index]];
	  	} else {
			return context->GetTensor(context, node->outputs->data[index]);
		  }
	",	
	"vulnerable_code_inside_full_function":"
		TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,
			int index) {
				if (context->tensors != nullptr) {
					return &context->tensors[node->outputs->data[index]];
				} else {
					return context->GetTensor(context, node->outputs->data[index]);
				}
			}

	",
	"revised_patch_highlighted":"
		TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,
			int index) {
			if (index >= 0 && index < node->outputs->size) {
				const int tensor_index = node->outputs->data[index];
				if (tensor_index != kTfLiteOptionalTensor) {
					if (context->tensors != nullptr) {
						return &context->tensors[tensor_index];
					} else {
						return context->GetTensor(context, tensor_index);
						}
					}
				}
				return nullptr;
			}


	"
------------------------------
	"example": 15,
    "id": "GHSA-p5f8-gfw5-33w4",
    "title": "Heap buffer overflow due to invalid splits in RaggedCountSparseOutput",
    "advisory_link": "https://github.com/advisories/GHSA-p5f8-gfw5-33w4",    
    "vulnerable_code_highlighted":"

		for (int idx = 0; idx < num_values; ++idx) { 
		while (idx >= splits_values(batch_idx)) { 
			batch_idx++; 
		} 
    ",
    "vulnerable_code_inside_full_function":"
		void Compute(OpKernelContext* context) override {
			const Tensor& splits = context->input(0);
			const Tensor& values = context->input(1);
			const Tensor& weights = context->input(2);
			bool use_weights = weights.NumElements() > 0;
			bool is_1d = false;

			const auto splits_values = splits.flat<int64>();
			const auto values_values = values.flat<T>();
			const auto weight_values = weights.flat<W>();
			int num_batches = splits.NumElements() - 1;
			int num_values = values.NumElements();

			auto per_batch_counts = BatchedMap<W>(num_batches);
			T max_value = 0;
			int batch_idx = 0;

			for (int idx = 0; idx < num_values; ++idx) {
			while (idx >= splits_values(batch_idx)) {
				batch_idx++;
			}
			const auto& value = values_values(idx);
			if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
				if (binary_output_) {
				per_batch_counts[batch_idx - 1][value] = 1;
				} else if (use_weights) {
				per_batch_counts[batch_idx - 1][value] += weight_values(idx);
				} else {
				per_batch_counts[batch_idx - 1][value]++;
				}
				if (value > max_value) {
				max_value = value;
				}
			}
			}

			int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);
			OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,
													is_1d, context));
		}

		private:
		int maxlength_;
		int minlength_;
		bool binary_output_;
		bool validate_;
		};
	",
	"revised_patch_highlighted":"
		OP_REQUIRES(
			context, num_batches > 0,
			errors::InvalidArgument(
				"Must provide at least 2 elements for the splits argument"));
		OP_REQUIRES(context, splits_values(0) == 0,
					errors::InvalidArgument("Splits must start with 0, not with ",
											splits_values(0)));
		OP_REQUIRES(context, splits_values(num_batches) == num_values,
					errors::InvalidArgument(
						"Splits must end with the number of values, got ",
						splits_values(num_batches), " instead of ", num_values));

		auto per_batch_counts = BatchedMap<W>(num_batches);
		T max_value = 0;
		int batch_idx = 0;

		for (int idx = 0; idx < num_values; ++idx) {
		while (idx >= splits_values(batch_idx)) {
			batch_idx++;
		}
	"
------------------------------
	"example": 16,
    "id": "GHSA-xmq7-7fxm-rr79",
    "title": "Format-string vulnerability in TensorFlow's `as_string`",
    "advisory_link": "https://github.com/advisories/GHSA-xmq7-7fxm-rr79",    
    "vulnerable_code_highlighted":"
		format_ = "%"; 
		if (width > -1) { 
		strings::Appendf(&format_, "%s%d", fill_string.c_str(), width); 
		} 
		if (precision > -1) { 
		strings::Appendf(&format_, ".%d", precision); 
		} 
    ",
    "vulnerable_code_inside_full_function":"

    explicit AsStringOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
    int32 precision;
    bool scientific;
    bool shortest;
    int32 width;
    string fill_string;
    DataType dtype;
    OP_REQUIRES_OK(ctx, ctx->GetAttr("T", &dtype));
    OP_REQUIRES_OK(ctx, ctx->GetAttr("precision", &precision));
    OP_REQUIRES_OK(ctx, ctx->GetAttr("scientific", &scientific));
    OP_REQUIRES_OK(ctx, ctx->GetAttr("shortest", &shortest));
    OP_REQUIRES_OK(ctx, ctx->GetAttr("width", &width));
    OP_REQUIRES_OK(ctx, ctx->GetAttr("fill", &fill_string));
    switch (dtype) {
      case DT_FLOAT:
      case DT_DOUBLE:
      case DT_COMPLEX64:
      case DT_COMPLEX128:
        break;
      default:
        OP_REQUIRES(ctx, !(scientific || shortest),
                    errors::InvalidArgument("scientific and shortest format "
                                            "not supported for datatype ",
                                            DataTypeString(dtype)));
        OP_REQUIRES(ctx, precision < 0,
                    errors::InvalidArgument("precision not supported "
                                            "for datatype ",
                                            DataTypeString(dtype)));
    }
    OP_REQUIRES(
        ctx, fill_string.size() <= 1,
        errors::InvalidArgument("Fill string must be one or fewer characters"));
    OP_REQUIRES(ctx, !(scientific && shortest),
                errors::InvalidArgument(
                    "Cannot select both scientific and shortest notation"));
    format_ = "%";
    if (width > -1) {
      strings::Appendf(&format_, "%s%d", fill_string.c_str(), width);
    }
    if (precision > -1) {
      strings::Appendf(&format_, ".%d", precision);
    }
    switch (dtype) {
      case DT_INT8:
      case DT_INT16:
      case DT_INT32:
        strings::Appendf(&format_, "d");
        break;
      case DT_INT64:
        strings::Appendf(&format_, "lld");
        break;
      case DT_FLOAT:
      case DT_DOUBLE:
      case DT_COMPLEX64:
      case DT_COMPLEX128:
        if (shortest) {
          strings::Appendf(&format_, "g");
        } else if (scientific) {
          strings::Appendf(&format_, "e");
        } else {
          strings::Appendf(&format_, "f");
        }
        break;
      case DT_BOOL:
        break;
      default:
        bool type_not_supported = true;
        OP_REQUIRES(ctx, !type_not_supported,
                    errors::InvalidArgument("Type not supported: ",
                                            DataTypeString(dtype)));
    }

    if (dtype == DT_COMPLEX64 || dtype == DT_COMPLEX128) {
      format_ = strings::Printf("(%s,%s)", format_.c_str(), format_.c_str());
    }
  }
	",
	"revised_patch_highlighted":"
		format_ = "%";
		if (!fill_string.empty()) {
		switch (fill_string[0]) {
			case ' ':
			case '+':
			case '-':
			case '0':
			case '#':
			strings::Appendf(&format_, "%s", fill_string.c_str());
			break;
			default:
			bool fill_not_supported = true;
			OP_REQUIRES(ctx, !fill_not_supported,
						errors::InvalidArgument("Fill argument not supported: \"",
												fill_string, "\""));
		}
		}
		if (width > -1) {
		strings::Appendf(&format_, "%d", width);
		}
		if (precision > -1) {
		strings::Appendf(&format_, ".%d", precision);
	"
------------------------------
	"example": 17,
    "id": "GGHSA-q4qf-3fc6-8x34",
    "title": "Segfault and data corruption caused by negative indexing in TFLite",
    "advisory_link": "https://github.com/advisories/GHSA-q4qf-3fc6-8x34",    
    "vulnerable_code_highlighted":"
		// Handle negative index. A positive index 'p_idx' can be represented as a 
		// negative index 'n_idx' as: n_idx = p_idx-num_dims 
		// eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  */ 
		int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx]; 
		TFLITE_DCHECK(current >= 0 && current < num_dims); 
    ",
    "vulnerable_code_inside_full_function":"
		inline bool ResolveAxis(const int num_dims, const int* axis,
								const int64_t num_axis, int* out_axis,
								int* out_num_axis) {
		*out_num_axis = 0;  // Just in case.
		// Short-circuit axis resolution for scalars; the axis will go unused.
		if (num_dims == 0) {
			return true;
		}
		// o(n^2) is fine since out_num_axis should be really small, mostly <= 4
		for (int64_t idx = 0; idx < num_axis; ++idx) {
			// Handle negative index. A positive index 'p_idx' can be represented as a
			// negative index 'n_idx' as: n_idx = p_idx-num_dims
			// eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  */
			int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];
			TFLITE_DCHECK(current >= 0 && current < num_dims);
			bool is_dup = false;
			for (int j = 0; j < *out_num_axis; ++j) {
			if (out_axis[j] == current) {
				is_dup = true;
				break;
			}
			}
			if (!is_dup) {
			out_axis[*out_num_axis] = current;
			*out_num_axis += 1;
			}
		}
		return true;
		}
	",
	"revised_patch_highlighted":"
		int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];
		TFLITE_DCHECK(current >= 0 && current < num_dims);
		if (current < 0 || current >= num_dims) {
		return false;
		}
	"
------------------------------
	"example": 18,
    "id": "GHSA-qc53-44cj-vfvx",
    "title": "Crash due to invalid splits in SparseCountSparseOutput",
    "advisory_link": "https://github.com/advisories/GHSA-qc53-44cj-vfvx",    
    "vulnerable_code_highlighted":"
		const auto indices_values = indices.matrix<int64>(); 
    ",
    "vulnerable_code_inside_full_function":"
		void Compute(OpKernelContext* context) override {
			const Tensor& indices = context->input(0);
			const Tensor& values = context->input(1);
			const Tensor& shape = context->input(2);
			const Tensor& weights = context->input(3);
			bool use_weights = weights.NumElements() > 0;

			bool is_1d = shape.NumElements() == 1;
			int num_batches = is_1d ? 1 : shape.flat<int64>()(0);
			int num_values = values.NumElements();

			const auto indices_values = indices.matrix<int64>();
			const auto values_values = values.flat<T>();
			const auto weight_values = weights.flat<W>();

			auto per_batch_counts = BatchedMap<W>(num_batches);

			T max_value = 0;

			for (int idx = 0; idx < num_values; ++idx) {
			int batch = is_1d ? 0 : indices_values(idx, 0);
			const auto& value = values_values(idx);
			if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
				if (binary_output_) {
				per_batch_counts[batch][value] = 1;
				} else if (use_weights) {
				per_batch_counts[batch][value] += weight_values(idx);
				} else {
				per_batch_counts[batch][value]++;
				}
				if (value > max_value) {
				max_value = value;
				}
			}
			}

			int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);
			OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,
													is_1d, context));
		}

		private:
		int maxlength_;
		int minlength_;
		bool binary_output_;
		bool validate_;
		};

	",
	"revised_patch_highlighted":"

		OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),
		errors::InvalidArgument(
			"Input indices must be a 2-dimensional tensor. Got: ",
			indices.shape().DebugString()));

			if (use_weights) {
			OP_REQUIRES(
			context, weights.shape() == values.shape(),
			errors::InvalidArgument(
			"Weights and values must have the same shape. Weight shape: ",
			weights.shape().DebugString(),
			"; values shape: ", values.shape().DebugString()));
			}

			bool is_1d = shape.NumElements() == 1;
			int num_batches = is_1d ? 1 : shape.flat<int64>()(0);
			int num_values = values.NumElements();

			OP_REQUIRES(context, num_values == indices.shape().dim_size(0),
				errors::InvalidArgument(
					"Number of values must match first dimension of indices.",
					"Got ", num_values,
					" values, indices shape: ", indices.shape().DebugString()));
			const auto indices_values = indices.matrix<int64>();

	"
------------------------------
	"example": 19,
    "id": "GHSA-9jjw-hf72-3mxw",
    "title": "Heap out of bounds read in filesystem glob matching",
    "advisory_link": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-9jjw-hf72-3mxw",    
    "vulnerable_code_highlighted":"
		auto handle_level = [fs, &results, &dir_q, &next_dir_q, &new_rets,
							&is_directory, &dirs, &results_mutex, &results_cond,
							&next_que_mutex, &next_que_cond](int i) {
			string current_dir = dir_q.at(i).first;
			int dir_index = dir_q.at(i).second;
			dir_index++;
			std::vector<string> children;
			Status s = fs->GetChildren(current_dir, &children);
			// In case PERMISSION_DENIED is encountered, we bail here.
			if (s.code() == tensorflow::error::PERMISSION_DENIED) {
				return;
		}

		if (!fs->Match(child_path, dirs[dir_index])) { ... }
    ",
    "vulnerable_code_inside_full_function":"
		while (!dir_q.empty()) {
			next_dir_q.clear();
			std::vector<Status> new_rets(dir_q.size());
			auto handle_level = [fs, &results, &dir_q, &next_dir_q, &new_rets,
								&is_directory, &dirs, &results_mutex, &results_cond,
								&next_que_mutex, &next_que_cond](int i) {
			string current_dir = dir_q.at(i).first;
			int dir_index = dir_q.at(i).second;
			dir_index++;
			std::vector<string> children;
			Status s = fs->GetChildren(current_dir, &children);
	",
	"revised_patch_highlighted":"
		std::deque<std::pair<string, int>> expand_queue;
		std::deque<std::pair<string, int>> next_expand_queue;
		expand_queue.emplace_back(dirs[matching_index - 1], matching_index - 1);
	
		// Adding to `result` or `new_expand_queue` need to be protected by mutexes
		// since there are multiple threads writing to these.
		mutex result_mutex;
		mutex queue_mutex;
	
		while (!expand_queue.empty()) {
		next_expand_queue.clear();
	
		// The work item for every item in `expand_queue`.
		// pattern, we process them in parallel.
		auto handle_level = [&fs, &results, &dirs, &expand_queue,
							&next_expand_queue, &result_mutex,
							&queue_mutex](int i) {
			// See invariants above, all of these are valid accesses.
			const auto& queue_item = expand_queue.at(i);
			const std::string& parent = queue_item.first;
			const int index = queue_item.second + 1;
			const std::string& match_pattern = dirs[index];
	
			// Get all children of `parent`. If this fails, return early.
			std::vector<std::string> children;
			Status s = fs->GetChildren(parent, &children);
	"
------------------------------
	"example": 20,
    "id": "GHSA-mxjj-953w-2c2v",
    "title": "Data corruption due to dimension mismatch in TFLite",
    "advisory_link": "https://github.com/advisories/GHSA-mxjj-953w-2c2v",    
    "vulnerable_code_highlighted":"
		TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2)); 
		return shape1.Dims(index1); 
    ",
    "vulnerable_code_inside_full_function":"
		// Get common shape dim, DCHECKing that they all agree.
		inline int MatchingDim(const RuntimeShape& shape1, int index1,
							const RuntimeShape& shape2, int index2) {
			TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
			return shape1.Dims(index1);
		}
	",
	"revised_patch_highlighted":"
		TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
		return std::min(shape1.Dims(index1), shape2.Dims(index2));
	"
------------------------------
	"example": 21,
    "id": "GHSA-hf5h-hh56-3vrg",
    "title": "Denial of Service in uws",
    "advisory_link": "https://github.com/advisories/GHSA-hf5h-hh56-3vrg",    
    "vulnerable_code_highlighted":"
	    do {
	        inflationStream.next_out = (Bytef *) inflationBuffer;
	        inflationStream.avail_out = LARGE_BUFFER_SIZE;
	        err = ::inflate(&inflationStream, Z_FINISH);
	        if (!inflationStream.avail_in) {
	            break;
	        }

	        dynamicInflationBuffer.append(inflationBuffer, LARGE_BUFFER_SIZE - inflationStream.avail_out);
	    } while (err == Z_BUF_ERROR);

	    inflateReset(&inflationStream);

	    if (err != Z_BUF_ERROR && err != Z_OK) {
	        length = 0;
	        return nullptr;
	    }
    ",
    "vulnerable_code_inside_full_function":"
		char *Hub::inflate(char *data, size_t &length) {
		dynamicInflationBuffer.clear();
		inflationStream.next_in = (Bytef *) data;
		inflationStream.avail_in = length;
		int err;
		do {
			inflationStream.next_out = (Bytef *) inflationBuffer;
			inflationStream.avail_out = LARGE_BUFFER_SIZE;
			err = ::inflate(&inflationStream, Z_FINISH);
			if (!inflationStream.avail_in) {
				break;
			}

			dynamicInflationBuffer.append(inflationBuffer, LARGE_BUFFER_SIZE - inflationStream.avail_out);
		} while (err == Z_BUF_ERROR);
		} while (err == Z_BUF_ERROR && dynamicInflationBuffer.length() <= INFLATE_LESS_THAN_ROUGHLY);

		inflateReset(&inflationStream);

		if (err != Z_BUF_ERROR && err != Z_OK) {
		if ((err != Z_BUF_ERROR && err != Z_OK) || dynamicInflationBuffer.length() > INFLATE_LESS_THAN_ROUGHLY) {
			length = 0;
			return nullptr;
		}
		if (dynamicInflationBuffer.length()) {
			dynamicInflationBuffer.append(inflationBuffer, LARGE_BUFFER_SIZE - inflationStream.avail_out);
			length = dynamicInflationBuffer.length();
			return (char *) dynamicInflationBuffer.data();
		}
		length = LARGE_BUFFER_SIZE - inflationStream.avail_out;
		return inflationBuffer;
		}
	"
	"revised_patch_highlighted":"

		static const int INFLATE_LESS_THAN_ROUGHLY = 16777216;

		do {
			inflationStream.next_out = (Bytef *) inflationBuffer;
			inflationStream.avail_out = LARGE_BUFFER_SIZE;
			err = ::inflate(&inflationStream, Z_FINISH);
			if (!inflationStream.avail_in) {
				break;
			}
        	dynamicInflationBuffer.append(inflationBuffer, LARGE_BUFFER_SIZE - inflationStream.avail_out);
    	} while (err == Z_BUF_ERROR && dynamicInflationBuffer.length() <= INFLATE_LESS_THAN_ROUGHLY);

	
	    if ((err != Z_BUF_ERROR && err != Z_OK) || dynamicInflationBuffer.length() > INFLATE_LESS_THAN_ROUGHLY) {
        length = 0;
        return nullptr;
    	}
	"
------------------------------
	"example": 22,
    "id": "GHSA-rjjg-hgv6-h69v",
    "title": "Memory corruption in Tensorflow",
    "advisory_link": "https://github.com/advisories/GHSA-rjjg-hgv6-h69v",    
    "vulnerable_code_highlighted":"
	    TFE_TensorHandle* thandle = EagerTensor_Handle(eager_tensor_pyobject_ptr);
    ",
    "vulnerable_code_inside_full_function":"
		m.def("TFE_ToDlpackCapsule", [](py::handle& o) {
	    PyObject* eager_tensor_pyobject_ptr = o.ptr();
	    TFE_TensorHandle* thandle = EagerTensor_Handle(eager_tensor_pyobject_ptr);
	    tensorflow::Safe_TF_StatusPtr status =
	        tensorflow::make_safe(TF_NewStatus());
	    void* dlm_ptr = tensorflow::TFE_HandleToDLPack(thandle, status.get());
	    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());

		TFE_TensorHandle* EagerTensor_Handle(const PyObject* o) {
			return reinterpret_cast<const EagerTensor*>(o)->handle;
		}
	",
	"revised_patch_highlighted":""
------------------------------
	"example": 23,
    "id": "GHSA-63xm-rx5p-xvqr",
    "title": "Heap buffer overflow in Tensorflow",
    "advisory_link": "https://github.com/advisories/GHSA-63xm-rx5p-xvqr",    
    "vulnerable_code_highlighted":"
		for (int i = 0; i < N; ++i) {
	      d_values(i) = grad_values(reverse_index_map(i));
	      visited(reverse_index_map(i)) = true;
	    }
    ",
    "vulnerable_code_inside_full_function":"
		void Compute(OpKernelContext* context) override {
		const Tensor* reverse_index_map_t;
		const Tensor* grad_values_t;
		OP_REQUIRES_OK(context,
					context->input("reverse_index_map", &reverse_index_map_t));
		OP_REQUIRES_OK(context, context->input("grad_values", &grad_values_t));

		const CPUDevice& d = context->eigen_device<CPUDevice>();

		OP_REQUIRES(
			context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),
			errors::InvalidArgument("reverse_index_map must be a vector, saw: ",
									reverse_index_map_t->shape().DebugString()));

		const auto reverse_index_map = reverse_index_map_t->vec<int64>();
		const auto grad_values = grad_values_t->vec<T>();

		const int64 N = reverse_index_map_t->shape().dim_size(0);
		const int64 N_full = grad_values_t->shape().dim_size(0);

		Tensor* d_values_t;
		OP_REQUIRES_OK(context, context->allocate_output(
									"d_values", TensorShape({N}), &d_values_t));
		auto d_values = d_values_t->vec<T>();
		Tensor* d_default_value_t;
		OP_REQUIRES_OK(context,
					context->allocate_output("d_default_value", TensorShape({}),
												&d_default_value_t));
		T& d_default_value = d_default_value_t->scalar<T>()();
		d_default_value = T();

		Tensor visited_t;
		OP_REQUIRES_OK(context, context->allocate_temp(
									DT_BOOL, TensorShape({N_full}), &visited_t));
		auto visited = visited_t.vec<bool>();
		visited.device(d) = visited.constant(false);

		for (int i = 0; i < N; ++i) {
		// Locate the index of the output of the forward prop associated
		// with this location in the input of the forward prop.  Copy
		// the gradient into it.  Mark it as visited.
		d_values(i) = grad_values(reverse_index_map(i));
		visited(reverse_index_map(i)) = true;
		}
		for (int j = 0; j < N_full; ++j) {
		// The default value gradient gets the accumulated remainder of
		// the backprop values (since the default value was used to fill
		// in these slots in the forward calculation).
		if (!visited(j)) {
			d_default_value += grad_values(j);
		}
		}
		}
	",
	"revised_patch_highlighted":"
	    OP_REQUIRES(context, TensorShapeUtils::IsVector(grad_values_t->shape()),
                errors::InvalidArgument("grad_values must be a vector, saw: ",
                                        grad_values_t->shape().DebugString()));
										
		int64 reverse_index = reverse_index_map(i);
		OP_REQUIRES(
		context, 0 <= reverse_index && reverse_index < N_full,
		errors::InvalidArgument("Elements in reverse index must be in [0, ",
							N_full, ") but got ", reverse_index));
		d_values(i) = grad_values(reverse_index);
		visited(reverse_index) = true;
	"
------------------------------
	"example": 24,
    "id": "GHSA-pg59-2f92-5cph",
    "title": "Heap buffer overflow in Tensorflow",
    "advisory_link": "https://github.com/advisories/GHSA-pg59-2f92-5cph",    
    "vulnerable_code_highlighted":"
        } else if (use_weights) {
          per_batch_counts[batch][value] += weight_values(idx);
        } else {
    ",
    "vulnerable_code_inside_full_function":"
		void Compute(OpKernelContext* context) override {
		const Tensor& indices = context->input(0);
		const Tensor& values = context->input(1);
		const Tensor& shape = context->input(2);
		const Tensor& weights = context->input(3);
		bool use_weights = weights.NumElements() > 0;

		bool is_1d = shape.NumElements() == 1;
		int num_batches = is_1d ? 1 : shape.flat<int64>()(0);
		int num_values = values.NumElements();

		const auto indices_values = indices.matrix<int64>();
		const auto values_values = values.flat<T>();
		const auto weight_values = weights.flat<W>();

		auto per_batch_counts = BatchedMap<W>(num_batches);

		T max_value = 0;

		for (int idx = 0; idx < num_values; ++idx) {
		int batch = is_1d ? 0 : indices_values(idx, 0);
		const auto& value = values_values(idx);
		if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
			if (binary_output_) {
			per_batch_counts[batch][value] = 1;
			} else if (use_weights) {
			per_batch_counts[batch][value] += weight_values(idx);
			} else {
			per_batch_counts[batch][value]++;
			}
			if (value > max_value) {
			max_value = value;
			}
		}
		}
	",
	"revised_patch_highlighted":"
		OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),
			errors::InvalidArgument(
				"Input indices must be a 2-dimensional tensor. Got: ",
				indices.shape().DebugString()));

		if (use_weights) {
			OP_REQUIRES(
				context, weights.shape() == values.shape(),
				errors::InvalidArgument(
					"Weights and values must have the same shape. Weight shape: ",
					weights.shape().DebugString(),
					"; values shape: ", values.shape().DebugString()));
			}

		OP_REQUIRES(context, num_values == indices.shape().dim_size(0),
			errors::InvalidArgument(
				"Number of values must match first dimension of indices.",
				"Got ", num_values,
				" values, indices shape: ", indices.shape().DebugString()));

	    if (use_weights) {
			OP_REQUIRES(
				context, weights.shape() == values.shape(),
				errors::InvalidArgument(
					"Weights and values must have the same shape. Weight shape: ",
					weights.shape().DebugString(),
					"; values shape: ", values.shape().DebugString()));
			}

		OP_REQUIRES(
			context, num_batches > 0,
			errors::InvalidArgument(
				"Must provide at least 2 elements for the splits argument"));
		OP_REQUIRES(context, splits_values(0) == 0,
			errors::InvalidArgument("Splits must start with 0, not with ",
									splits_values(0)));
		OP_REQUIRES(context, splits_values(num_batches) == num_values,
			errors::InvalidArgument(
				"Splits must end with the number of values, got ",
				splits_values(num_batches), " instead of ", num_values));
	"
------------------------------
	"example": 25,
    "id": "GHSA-q263-fvxm-m5mw",
    "title": "Heap out of bounds access in MakeEdge in TensorFlow",
    "advisory_link": "https://github.com/advisories/GHSA-q263-fvxm-m5mw",    
    "vulnerable_code_highlighted":"
		DataType src_out = src->output_type(output_index);
		DataType dst_in = dst->input_type(input_index);
    ",
    "vulnerable_code_inside_full_function":"
		Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,
										int input_index) {

		DataType src_out = src->output_type(output_index);
		DataType dst_in = dst->input_type(input_index);
		if (!TypesCompatible(dst_in, src_out)) {
			return errors::InvalidArgument(
				"Input ", input_index, " of node ", dst->name(), " was passed ",
				DataTypeString(src_out), " from ", src->name(), ":", output_index,
				" incompatible with expected ", DataTypeString(dst_in), ".");
		}
		g_->AddEdge(src, output_index, dst, input_index);
		return Status::OK();
		}
	",
	"revised_patch_highlighted":"
		if (output_index >= src->num_outputs()) {
			return errors::InvalidArgument(
			"Output ", output_index, " of node ", src->name(),
			" does not exist. Node only has ", src->num_outputs(), " outputs.");
		}
		if (input_index >= dst->num_inputs()) {
			return errors::InvalidArgument(
			"Input ", input_index, " of node ", dst->name(),
			" does not exist. Node only has ", dst->num_inputs(), " inputs.");
		}
	"
------------------------------
	"example": 26,
    "id": "GHSA-rrfp-j2mp-hq9c",
    "title": "Segfault in tf.quantization.quantize_and_dequantize",
    "advisory_link": "https://github.com/advisories/GHSA-rrfp-j2mp-hq9c",    
    "vulnerable_code_highlighted":"
		const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);
    ",
    "vulnerable_code_inside_full_function":"
		void Compute(OpKernelContext* ctx) override {
		const Tensor& input = ctx->input(0);
		const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);
		Tensor input_min_tensor;
		Tensor input_max_tensor;
		Tensor* output = nullptr;
		OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));
		if (range_given_) {
		input_min_tensor = ctx->input(1);
		input_max_tensor = ctx->input(2);
		if (axis_ == -1) {
			auto min_val = input_min_tensor.scalar<T>()();
			auto max_val = input_max_tensor.scalar<T>()();
			OP_REQUIRES(ctx, min_val <= max_val,
						errors::InvalidArgument("Invalid range: input_min ",
												min_val, " > input_max ", max_val));
		} else {
			OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,
						errors::InvalidArgument(
							"input_min_tensor has incorrect size, was ",
							input_min_tensor.dim_size(0), " expected ", depth,
							" to match dim ", axis_, " of the input ",
							input_min_tensor.shape()));
			OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,
						errors::InvalidArgument(
							"input_max_tensor has incorrect size, was ",
							input_max_tensor.dim_size(0), " expected ", depth,
							" to match dim ", axis_, " of the input ",
							input_max_tensor.shape()));
		}
		} else {
		auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});
		OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,
												range_shape, &input_min_tensor));
		OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,
												range_shape, &input_max_tensor));
		}
		if (axis_ == -1) {
		functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f;
		f(ctx->eigen_device<Device>(), input.flat<T>(), signed_input_, num_bits_,
			range_given_, &input_min_tensor, &input_max_tensor, round_mode_,
			narrow_range_, output->flat<T>());
		} else {
		functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f;
		f(ctx->eigen_device<Device>(),
			input.template flat_inner_outer_dims<T, 3>(axis_ - 1), signed_input_,
			num_bits_, range_given_, &input_min_tensor, &input_max_tensor,
			round_mode_, narrow_range_,
			output->template flat_inner_outer_dims<T, 3>(axis_ - 1));
		}
		}
	",
	"revised_patch_highlighted":"
		OP_REQUIRES(
        ctx, (axis_ == -1 || axis_ < input.shape().dims()),
        errors::InvalidArgument("Shape must be at least rank ", axis_ + 1,
                                " but is rank ", input.shape().dims()));
	"
------------------------------
	"example": 27,
    "id": "GHSA-hhvc-g5hv-48c6",
    "title": "Write to immutable memory region in TensorFlow",
    "advisory_link": "https://github.com/advisories/GHSA-hhvc-g5hv-48c6",    
    "vulnerable_code_highlighted":"
	template <>
	/* static */
	inline void TypedAllocator::RunCtor(Allocator* raw_allocator, tstring* p,
	                                    size_t n) {
	  if (!raw_allocator->AllocatesOpaqueHandle()) {
	    for (size_t i = 0; i < n; ++p, ++i) new (p) tstring();
	  }
	}
    ",
    "vulnerable_code_inside_full_function":"
		inline void TypedAllocator::RunCtor(Allocator* raw_allocator, tstring* p,
											size_t n) {
		if (!raw_allocator->AllocatesOpaqueHandle()) {
			for (size_t i = 0; i < n; ++p, ++i) new (p) tstring();
		}
		}
	",
	"revised_patch_highlighted":"
		bool AllocatesOpaqueHandle() const override { return true; }
	"
------------------------------
	"example": 28,
    "id": "GHSA-hfvc-g252-rp4g",
    "title": "Denial of Service in i18n",
    "advisory_link": "https://github.com/advisories/GHSA-hfvc-g252-rp4g",    
    "vulnerable_code_highlighted":"
		if (LocalizedApplication.Current.MessageKeyIsValueInDefaultLanguage
			&& LocalizedApplication.Current.DefaultLanguageTag.Equals(langtag)) {
			return true; }
    "
    "vulnerable_code_inside_full_function":"
		private bool IsLanguageValid(string langtag)
        {
        // Note that there is no need to serialize access to System.Web.HttpRuntime.Cache when just reading from it.
        //
            // Default language is always valid.
            if (LocalizedApplication.Current.MessageKeyIsValueInDefaultLanguage
                && LocalizedApplication.Current.DefaultLanguageTag.Equals(langtag)) {
                return true; }
            ConcurrentDictionary<string, TranslationItem> messages = (ConcurrentDictionary<string, TranslationItem>)System.Web.HttpRuntime.Cache[GetCacheKey(langtag)];
            // If messages not yet loaded in for the language
            if (messages == null)
            {
                return _translationRepository.TranslationExists(langtag);
            }
            return true;
        }
	",
	"revised_patch_highlighted":""
------------------------------
	"example": 29,
    "id": "GHSA-vwcg-7xqw-qcxw",
    "title": "Heap Overflow in PyMiniRacer",
    "advisory_link": "https://github.com/advisories/GHSA-vwcg-7xqw-qcxw",    
    "vulnerable_code_highlighted":"
		else if (value->IsArray()) {
        Local<Array> arr = Local<Array>::Cast(value);
        size_t len = arr->Length();

        BinaryValue **ary = xalloc(ary, sizeof(*ary) * len);

        res->type = type_array;
        res->array_val = ary;

        for(uint32_t i = 0; i < arr->Length(); i++) {
            Local<Value> element = arr->Get(context, i).ToLocalChecked();
            BinaryValue *bin_value = convert_v8_to_binary(isolate, context, element);
            if (bin_value == NULL) {
                goto err;
            }
            ary[i] = bin_value;
            res->len++;
        }
    	}
    ",
    "vulnerable_code_inside_full_function":"
		static BinaryValue *convert_v8_to_binary(Isolate * isolate,
								Local<Context> context,
										Local<Value> value)
		{
			Isolate::Scope isolate_scope(isolate);
			HandleScope scope(isolate);
			BinaryValue *res = new (xalloc(res)) BinaryValue();
			if (value->IsNull() || value->IsUndefined()) {
				res->type = type_null;
			}
			else if (value->IsInt32()) {
				res->type = type_integer;
				auto val = value->Uint32Value(context).ToChecked();
				res->int_val = val;
			}
			// ECMA-262, 4.3.20
			// http://www.ecma-international.org/ecma-262/5.1/#sec-4.3.19
			else if (value->IsNumber()) {
				res->type = type_double;
				double val = value->NumberValue(context).ToChecked();
				res->double_val = val;
			}
			else if (value->IsBoolean()) {
				res->type = type_bool;
				res->int_val = (value->IsTrue() ? 1 : 0);
			}

			else if (value->IsArray()) {
				Local<Array> arr = Local<Array>::Cast(value);
				size_t len = arr->Length();

				BinaryValue **ary = xalloc(ary, sizeof(*ary) * len);

				res->type = type_array;
				res->array_val = ary;

				for(uint32_t i = 0; i < arr->Length(); i++) {
					Local<Value> element = arr->Get(context, i).ToLocalChecked();
					BinaryValue *bin_value = convert_v8_to_binary(isolate, context, element);
					if (bin_value == NULL) {
						goto err;
					}
					ary[i] = bin_value;
					res->len++;
				}
			}

			else if (value->IsFunction()){
				res->type = type_function;
			}
			else if (value->IsSymbol()){
				res->type = type_symbol;
			}
			else if (value->IsDate()) {
				res->type = type_date;
				Local<Date> date = Local<Date>::Cast(value);
				double timestamp = date->ValueOf();
				res->double_val = timestamp;
			}
			else if (value->IsObject()) {
				res->type = type_hash;
				TryCatch trycatch(isolate);
				Local<Object> object = value->ToObject(context).ToLocalChecked();
				MaybeLocal<Array> maybe_props = object->GetOwnPropertyNames(context);
				if (!maybe_props.IsEmpty()) {
					Local<Array> props = maybe_props.ToLocalChecked();
					uint32_t hash_len = props->Length();
					if (hash_len > 0) {
						res->hash_val = xalloc(res->hash_val,
											sizeof(*res->hash_val) * hash_len * 2);
					}
					for (uint32_t i = 0; i < hash_len; i++) {
						MaybeLocal<Value> maybe_pkey = props->Get(context, i);
						if (maybe_pkey.IsEmpty()) {
					goto err;
				}
				Local<Value> pkey = maybe_pkey.ToLocalChecked();
						MaybeLocal<Value> maybe_pvalue = object->Get(context, pkey);
						// this may have failed due to Get raising
						if (maybe_pvalue.IsEmpty() || trycatch.HasCaught()) {
							// TODO: factor out code converting exception in
							//       nogvl_context_eval() and use it here/?
							goto err;
						}
						BinaryValue *bin_key = convert_v8_to_binary(isolate, context, pkey);
						BinaryValue *bin_value = convert_v8_to_binary(isolate, context, maybe_pvalue.ToLocalChecked());
						if (!bin_key || !bin_value) {
							BinaryValueFree(bin_key);
							BinaryValueFree(bin_value);
							goto err;
						}
						res->hash_val[i * 2]     = bin_key;
						res->hash_val[i * 2 + 1] = bin_value;
						res->len++;
					}
				} // else empty hash
			}
			else {
				Local<String> rstr = value->ToString(context).ToLocalChecked();
				res->type = type_str_utf8;
				res->len = size_t(rstr->Utf8Length(isolate)); // in bytes
				size_t capacity = res->len + 1;
				res->str_val = xalloc(res->str_val, capacity);
				rstr->WriteUtf8(isolate, res->str_val);
			}
			return res;
		err:
			BinaryValueFree(res);
			return NULL;
		}
	",
	"revised_patch_highlighted":""
------------------------------
	"example": 30,
    "id": "GHSA-85rr-4rh9-hhwh",
    "title": "Memory leak in Nanopb",
    "advisory_link": "https://github.com/advisories/GHSA-85rr-4rh9-hhwh",    
    "vulnerable_code_highlighted":"
            *(pb_size_t*)iter->pSize = iter->pos->tag;
            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE)
            {
                memset(iter->pData, 0, iter->pos->data_size);
                pb_message_set_to_defaults((const pb_field_t*)iter->pos->ptr, iter->pData);
            }

            return func(stream, iter->pos, iter->pData);
    ",
    "vulnerable_code_inside_full_function":"
		static bool checkreturn decode_static_field(pb_istream_t *stream, pb_wire_type_t wire_type, pb_field_iter_t *iter)
		{
			pb_type_t type;
			pb_decoder_t func;
			
			type = iter->pos->type;
			func = PB_DECODERS[PB_LTYPE(type)];
			switch (PB_HTYPE(type))
			{
				case PB_HTYPE_REQUIRED:
					return func(stream, iter->pos, iter->pData);
					
				case PB_HTYPE_OPTIONAL:
					if (iter->pSize != iter->pData)
						*(bool*)iter->pSize = true;
					return func(stream, iter->pos, iter->pData);
			
				case PB_HTYPE_REPEATED:
					if (wire_type == PB_WT_STRING
						&& PB_LTYPE(type) <= PB_LTYPE_LAST_PACKABLE)
					{
						/* Packed array */
						bool status = true;
						pb_size_t *size = (pb_size_t*)iter->pSize;
						pb_istream_t substream;
						if (!pb_make_string_substream(stream, &substream))
							return false;
						while (substream.bytes_left > 0 && *size < iter->pos->array_size)
						{
							void *pItem = (char*)iter->pData + iter->pos->data_size * (*size);
							if (!func(&substream, iter->pos, pItem))
							{
								status = false;
								break;
							}
							(*size)++;
						}
						if (substream.bytes_left != 0)
							PB_RETURN_ERROR(stream, "array overflow");
						if (!pb_close_string_substream(stream, &substream))
							return false;
						return status;
					}
					else
					{
						/* Repeated field */
						pb_size_t *size = (pb_size_t*)iter->pSize;
						char *pItem = (char*)iter->pData + iter->pos->data_size * (*size);
						if ((*size)++ >= iter->pos->array_size)
							PB_RETURN_ERROR(stream, "array overflow");
						return func(stream, iter->pos, pItem);
					}

				case PB_HTYPE_ONEOF:
					*(pb_size_t*)iter->pSize = iter->pos->tag;
					if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE)
					{
						memset(iter->pData, 0, iter->pos->data_size);
						pb_message_set_to_defaults((const pb_field_t*)iter->pos->ptr, iter->pData);
					}

					return func(stream, iter->pos, iter->pData);

				default:
					PB_RETURN_ERROR(stream, "invalid field type");
			}
		}
	",
	"revised_patch_highlighted":"
		case PB_HTYPE_ONEOF:
		if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE &&
			*(pb_size_t*)iter->pSize != iter->pos->tag)
		{
			/* We memset to zero so that any callbacks are set to NULL.
				* This is because the callbacks might otherwise have values
				* from some other union field. */
			memset(iter->pData, 0, iter->pos->data_size);
			pb_message_set_to_defaults((const pb_field_t*)iter->pos->ptr, iter->pData);
		}
		*(pb_size_t*)iter->pSize = iter->pos->tag;

		return func(stream, iter->pos, iter->pData);
	"
------------------------------